# AI基础01-数学基础
## 1.回归
回归就是多项式曲线拟合，但是次数过低无法很好的拟合，次数过高又会出现过拟合，因此用一些算法来优化回归：
#### 正则化
常用的正则化有L1正则化和L2正则化
## 2.概率论
#### 频率学派与贝叶斯学派
贝叶斯定理：
$$p(Y|X)(后验posterior) = \frac{p(X|Y)(似然likelihood) * p(Y)(先验prior)}{\sum_{Y}  p(X|Y)p(Y)(归一化常量)}$$

$$posterior \propto likelihood * prior$$
#### 期望、方差、协方差
此处将方差定义为$Var(f) = E(f^2)-E^2(f)$，则自然的引出协方差的定义$Cov(x, y) = E(xy^T)-E(x)E(y^T)$。
#### 高斯分布
$$\mathbb { E } [ x ] = \int _ { - \infty } ^ { \infty } \mathcal { N } \left( x \mid \mu , \sigma ^ { 2 } \right) x \mathrm { ~ d } x = \mu
$$
$$
\mathbb { E } \left[ x ^ { 2 } \right] = \int _ { - \infty } ^ { \infty } \mathcal { N } \left( x \mid \mu , \sigma ^ { 2 } \right) x ^ { 2 } \mathrm { ~ d } x = \mu ^ { 2 } + \sigma ^ { 2 }
$$
$$
 \operatorname { v a r } [ x ] = \mathbb { E } \left[ x ^ { 2 } \right] - \mathbb { E } [ x ] ^ { 2 } = \sigma ^ { 2 }
$$

#### 最大似然估计（MLE）
取似然函数的负对数作为误差函数，似然函数的积变成误差函数的和，最大化似然函数就是最小化误差函数。
**推导：高斯分布在MLE中的参数估计**
观察数据集$\mathbf{x} = (x_{1}, \dots, x_{n})^T$满足独立同分布(i.i.d)，对于满足i.i.d的变量，有联合概率$p(\mathbf{x}) = \prod N(x_n|\mu, \sigma^2)$ ，最大化$p(\mathbf x)$ 可以找到最佳的$\mu$和$\sigma$。
最大化的方法是取对数之后分别对$\mu$和$\sigma$求偏导并令导数等于0，最终可以得到$\mu _{M⁢L} = \frac{1}{N}\sum _{n = 1}^{N}x_{n}$，$\sigma _{M⁢L}^{2} = \frac{1}{N}\sum _{n = 1}^{N}\left(x_{n}−\mu _{M⁢L}\right)2$  
#### 最大后验概率（MAP）
## 3.信息论
#### 自信息Self-Infomation
$$I(x)=-lnP(x)$$
单位是纳特nats
### 香农熵Shannon-Entropy
自信息只能衡量一个事件的信息量，香农熵用来衡量整个概率分布的不确定性。
$$H ( \mathrm { x } ) = \mathbb { E } _ { \mathrm { X } \sim P } [ I ( x ) ] = - \mathbb { E } _ { \mathrm { X } \sim P } [ \log P ( x ) ]
$$

### KL-散度(KL-divergence)与交叉熵(cross entropy)