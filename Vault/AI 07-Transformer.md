# AI基础07-Transformer
目前的处理序列问题的工具：
- RNN：长序列会梯度消失/梯度爆炸
- LSTM/GRU：计算效率较低
- CNN：感受野有限，不能捕捉全局信息
### 人类注意力机制
心理学：非自主性提示、自主性提示
灵长类视觉系统分层级的信号处理
- 低层级（视网膜）：视觉编码
- 中层级（视皮层）：视觉解码
- 高层级（其他脑区）：理解与感知
灵长类中央凹(20%)与注意力机制有关，外周视觉(80%)与高效编码有关。
### 自注意力机制 Self-attention
注意力提示：
- 非自住性提示：Key, Value
- 自主性提示：Query
- 注意力汇聚：打分函数（缩放点积注意力、加性注意力）、注意力权重矩阵、加权平均
将输入序列的线性表示（矩阵）通过三个不同的线性层投影分别生成查询矩阵Q，键矩阵K和值矩阵V，用于计算注意力输出。
### 多头注意力
h个self-attention的组合，类似于神经网络全连接层堆叠神经元
### Transformer
### 常用的预训练模型
GPT、BERT