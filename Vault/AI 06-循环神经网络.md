# AI基础06-循环神经网络
用于处理时间序列数据集，与之相对的是前馈神经网络。
## 词的表示
### One-hot vector
过于原始，维数灾难。
### 词袋模型 Bag of Words
在计算机视觉中常用。
在RNN中可能有维数灾难，且丢失位置信息。
### 词嵌入 Word Embedding
用一组浮点数向量来表示单词。
#### 优点
- 维度低
- 用相近向量代表相近词义，甚至可以用向量加减表示语义(?)
- 密集向量容易计算
#### 习得词嵌入
通过数据学习词嵌入表，是一种自监督学习(self-supervised learning)
#### 具体算法
**Word2Vec = Skip-Gram / CBOW+ Negative Sampling(负采样技术)**
（Skip-Gram是通过中间词预测上下文，CBOW是通过上下文预测中间词。）
**噪声对比估计（NCE）**
## 序列数据 Sequential Data
### 时间步
序列数据的重要概念，每个单词可以看作一个时间步，时间步按照单词在句子中你出现的先后顺序排序，表示句子中单词的序列信息。
### 一些用途
- one to many: 输入图片，输出图片描述
- many to one: 输入句子，输出情感分析数值
- 异步many to many: 语言翻译，再开始翻译之前拥有整个句子；交通计数，实施输入视频
- 同步many to many: 天气预测，每个时间步都有输入和输出；聊天机器人(！)
### Vanilla RNN
将前一个时间步的信息传给下一个时间步。
局限性：很难维持较为长期的信息。
### 长短期记忆网络 LSTM(Long Short-Term Memory)
#### 门控单元
门控函数通过将输入向量与门控向量逐元素相乘来过滤信息。
遗忘门、输入门、输出门
大部分时候使用Sigmoid函数，也有时候使用ReLU或tanh。特殊的，输入门需要用tanh以获得更好的特征表达。
### LSTM变体：门控循环单元 GRU
不具有cell state，减少了LSTM的计算成本和内存使用。
并没有提供显著改进。