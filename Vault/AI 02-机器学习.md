# AI基础02-机器学习
**三要素**：任务、经验、表现
**分类**：有监督学习、无监督学习、半监督学习、弱监督学习、强化学习
（机器学习分类和一些经典的任务可见a神笔记）
## 1. 线性回归(Linear Regression)
$$\mathbf{y} = \beta\mathbf{x} + \sigma$$
#### 模型介绍
通过最小化训练集误差来取得经验均值(Empirical Mean)，$f ^ { * } = \arg \min _ { f } \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \left( f \left( x _ { i } \right) - y _ { i } \right) ^ { 2 }$
### 模型求解
对$A_{n*p}$进行最小二乘法，因为有n个训练样本，每个训练样本有p个维度。
- 如果n远大于p，可以认为$A^TA$是可逆的，通过偏导等于0解出来$\hat{\beta} = (A^TA)^{-1}A^TY$，则称此式有闭式(closed form)解。但n太大时直接求解并不合适，此时可以用梯度下降来求解。
**梯度下降**
每次沿着梯度的反方向走learning-rate的长度，只要损失函数是凸函数，就总能走到最优解。
- 如果n小于p，$A^TA$不可逆，则可以为$\beta$假设先验分布prior distribution，后验概率=先验概率$\times$似然函数。
对函数的先验假设的过程就是正则化的过程。
### 正则化
对于高斯分布作为先验假设的最大后验概率(MAP)经推导后得到的结果是：
$$\hat { \beta } _ { \mathrm { M A P } } = \arg \min \sum _ { i = 1 } ^ { n } \left( Y _ { i } - X _ { i } \beta \right) ^ { 2 } + \lambda \| \beta \| _ { 2 } ^ { 2 }$$
从max变成min的原因是从最大化概率变成了最小化误差。
其中$\lambda \| \beta \| _ { 2 } ^ { 2 }$是正则项（惩罚项），随先验假设的变化而变化。
$\lambda$是一个超参数，作用是控制对参数的偏好，防止过拟合。

推广到通用正则化项之后得到：
$$\min _ { \beta } ( \mathbf { A } \beta - \mathbf { Y } ) ^ { T } ( \mathbf { A } \beta - \mathbf { Y } ) + \lambda \operatorname { p e n } ( \beta ) = \min J ( \beta ) + \lambda \operatorname { p e n } ( \beta)$$
其中$J(\beta)$是损失函数，$pen(\beta)$是惩罚项。
对于惩罚项一半有L1和L2正则化两种选择：
- L1正则化（1-范数）：$\operatorname { p e n } ( \beta ) = \| \beta \| _ { 1 } = \sum _ { i = 1 } ^ { n } \left| \beta _ { i } \right|$，选择拉普拉斯分布作为先验分布。
- L2正则化（2-范数）：$\operatorname { p e n } ( \beta ) = \| \beta \| _ { 2 } ^ { 2 } = \sum _ { i = 1 } ^ { n } \beta _ { i } ^ { 2 }$，选择高斯分布作为先验分布。
其解有以下特点：
- L1套索回归的正则化项： $\lambda \sum _ { j = 1 } ^ { p } \left| \beta _ { j } \right| ,$ 这是一个L1范数，它对所有系数施加相同的惩罚，这会导致一些系数直接为零，从而产生一个**稀疏解**。**非零 $w$ 更少**。 
- L2岭回归的正则化项： $\lambda \sum _ { j = 1 } ^ { p } \beta _ { j } ^ { 2 }$, 这是一个L2范数，它对大的系数施加更大的惩罚，导致系数**平滑地趋近于零**。**有些 $w$ 更小**。
![[attachments/Pasted image 20250419170637.png]]
## 2. 逻辑回归(Logistic Regression)
#### Sigmoid函数
$\sigma(x) = -\frac{1}{1+e^{-(\omega^Tx+b)}}$
把它作为网格输出$C_1$的后验概率，即$P(C_1|\mathbf{x})=-\frac{1}{1+e^{-(\omega^Tx+b)}}$

后面的就听不懂了。