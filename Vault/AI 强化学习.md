# 强化学习
Sutton RLBook
## 第1章 导论
强化学习(Reinforcement Learning)是一种跟人类一样，通过环境的交互和反馈来进行学习。
### 1. 马尔可夫过程
![强化学习环境示意图](attachments/Pasted%20image%2020250608103854.png)

### 2. 早期历史
- 第一条主线是源于动物心理学的试错法
- 第二条主线是关注最优控制问题以及使用价值函数和动态规划的解决方案。
- 第三条主线是时序差分方法。
## 第2章 多臂老虎机
多臂老虎机是经典的强化学习模型，其特点在于无监督学习（自主学习），只依赖环境的反馈。
### 1. 问题建模
- 通过价值估计动作的选择，称为”动作-价值方法“
$$A_t \doteq argmax_a Q_t(a)$$
- 设计增量式公式以小而恒定的计算来更新平均值
$$Q_{n+1} = Q_n + \frac1n[R_n-Q_n]$$
	其中$R_n$是第n次的收益
### 2. 算法及优化
- 纯贪心、$\epsilon$-贪心
- 乐观初值
- UCB(Upper Confidence Bound)
- 梯度强盗算法
## 第3章 有限马尔可夫决策过程
相比于老虎机只需要关注即时收益，大多数问题更需要关心延迟收益。对于这些问题，我们可以使用MDP进行建模。
### 1. 有限MDP(Markov Decision Procedure)
![[attachments/Pasted image 20251006142310.png]]
- 目标被形式化表征为一种特殊信号，称为收益。
在这个模型下，收益可以被表示为：
$$p(s', r|s, a)\doteq Pr\{S_t=s', R_t=r|S_{t-1}=s, A_{t-1}=a\}$$
其中Pr表示Probability。
- 我们希望最大化回报，而回报与未来的收益序列有关。
一般将回报定义为
$$G_t \doteq R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\dots=\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}$$
$$G_t\doteq R_{t+1}+\gamma G_{t+1}$$
其中$\gamma$称为折扣率
- 智能体和环境的交互具有马尔可夫性，有限MDP中下一个状态仅由有限个状态决定。我们可以用状态转移概率表示。
$$p(s'|s, a)\doteq \mathbb{E}[R_t|S_{t-1}=s, A_{t-1}=a]=\sum_{r\in \mathcal{R}}p(s', r|s, a)$$
- 我们还可以定义“状态-动作”二元组的期望收益，并将其表示为一个双参数函数：
$$r(s, a)\doteq E[R_t|S_{t-1}=s, A_{t-1}=a]$$
和“状态-动作-后继状态”三元组的期望收益，并将其表示为一个三参数函数：
$$r(s, a, s')\doteq \mathbb{E}[R_t|S_{t-1}=s, A_{t-1}=a, S_t=s']=\sum_{r\in \mathcal{R}}r\frac{p(s', r|s, a)}{p(s'|s, a)}$$
- MDP框架将智能体与环境之间传递的所有信息看作3个信号：行动、状态、收益。
任务分为分幕式任务和持续性任务。我们定义“吸收态”来统一这两种类型的任务。吸收态只能转移到自己且收益为0。这样就只需要研究分幕式任务。而在分幕式任务中，不必区分不同的幕，$S_t$表示的是每一幕中t时刻的状态。
### 2. 策略与价值
- 策略：状态到可行的动作的一个分布函数，用$\pi(a|s)$表示$S_t = s$时$A_t = a$的概率。而强化学习就是通过策略来修改$\pi$。此外，我们用$v_\pi(s)$来表示策略$\pi$的状态价值，用$q_\pi(s)$来表示策略$\pi$的动作价值。
![[attachments/Pasted image 20251007131155.png]]
可以通过蒙特卡洛方法计算这两个值。（算出来、存起来）
#### 贝尔曼方程
原理：当智能体采取策略$\pi$，并对于每个状态都记录实际回报的平均值，那么当状态出现的次数趋于无穷时，这个平均值就会收敛到$v_{\pi}(s)$，同理动作的回报的平均值收敛到$q_{\pi}(s, a)$。
于是我们可以用蒙特卡洛方法来求出这两者的值。

具体的，贝尔曼期望方程描述了任何策略$\pi$和任何状态s，s的价值与其后继状态价值的关系。
另外，现在我们研究的是一个既定策略的价值，但我们期望得到的是一个最优策略。**贝尔曼最优方程**阐述了一个事实：最优状态下各个状态的价值一定等于这个状态下最优动作的期望价值。
![[attachments/Pasted image 20251007133840.png]]
解释：**贝尔曼期望方程**来源于$v_{\pi}(x)$的定义，第一个Sigma是对当前状态每种动作的求和，第二个Sigma是对给定动作可能结果的求和，求和的内容是回报的加权平均。
**贝尔曼最优方程**将期望方程中的第一个Sigma换成了max，这意味着v不再收敛到给定策略的价值，而是收敛到整个问题的最优策略的价值。

贝尔曼方程的意义在于，我们可以将最优的长期（全局）回报期望值转化为每个状态对应的一个当前局部量的计算。动作价值函数$q*(s, a)$包含着所有单步搜索的结果，智能体只需要找到q* ，最优动作的选取就不再需要知道后继状态及其对应的价值了，也就是说不需要再与动态环境进行交互了。

理论上，贝尔曼方程组有n个方程和n个未知数，可以直接解出最优策略。但现实问题往往无法求出闭式解，我们更关注问题的近似解。

## 第4章 动态规划
> In fact, all of these methods can be viewed as attempts to achieve much the same effect as DP, only with less computation and without assuming a perfect model of the environment.
 ——RLBook
 
 在强化学习中，DP的核心思想是使用价值函数来结构化的组织对最优策略的探索。
### 1. 通过策略改进得到最优策略
1. 策略评估 Policy Evaluation
	我们希望对于策略$\pi$计算$v_{\pi}$:
	- 迭代策略评估：通过迭代近似的价值函数序列得到贝尔曼方程的近似解。在保证$v_\pi$存在的条件下，序列$v_k$将会在$k\rightarrow\infty$时收敛到$v_\pi$。
	- 期望更新：通过单步转移后的即时收益和$s$的每个状态（或状态-动作二元组）的旧的价值函数二者的期望值来更新$s$的新的价值函数。
2. 策略改进 Policy Improvement
	我们可以通过策略评估进行策略改进，策略改进定理表示每一次改进都不会比先前更差，由状态的有限性可以得到策略改进算法一定可以在有限步内收敛。
3. 策略迭代 Policy Iteration
	- 策略迭代：交替进⾏迭代策略估值和策略提升，在有限步之后找到最优策略与最优值函数。
### 2. 通过最最优状态价值得到最优策略
1. 值迭代
	- 是一种提升策略迭代效率的方法。策略迭代在策略评估上浪费了大量时间，我们想到可以提前结束估值。
	- 值迭代是一种极端的矫正方式，只做一次估值就开始策略提升。
	- 实际问题中可以将两者结合，每值迭代若干轮做一次策略提升。
### 3. 提升求解效率
1. 异步动态规划
	![[attachments/Pasted image 20251017103144.png]]
2. 广义策略迭代 Generalized Policy Iteration(GPI)
	- 几乎所有的强化学习方法都可以用GPI描述。
	- 与过程的粒度或其他细节无关。
	![[attachments/Pasted image 20251017103705.png]]
		3. 动态规划效率
		- 对初始值依赖性比较大
		- 假设环境已知(P, R已知)
		- 使用bootstrap（状态的估值是根据其后继状态估值的）
## 第5章 蒙特卡洛方法
蒙特卡洛方法不假设有完备的环境知识，而是只需要经验。
### 1. 蒙特卡洛预测和价值评估
- 蒙特卡洛预测有一个重要前提：对每个状态的估计是独立的。
- 如果无法得到环境模型，那么计算动作价值就比计算状态价值更加有用一些。我们可以用状态-动作二元组的价值$q_\pi(s, a)$来表示动作价值。
- 直接蒙特卡洛会遇到问题，一部分状态可能永远没有被访问到。
	1. 引入“探索性起步”，指定开始采样的状态，这样可以保证每个状态都收敛。
	2. 保证每个状态下所有动作都有非零概率被选中，也就是$\epsilon$-soft策略。
### 2. 蒙特卡洛控制
#### 蒙特卡洛ES
”试探性出发“假设是指在采样次数趋向于无穷时，每个状态-动作二元组都会被访问无数次。
另外一个假设是在进行策略评估时有无限多幕的样本序列进行试探。
满足这两个假设就可以直接使用蒙特卡洛算法，但大多数情况下不满足，因此我们需要用某些方法来去除这两个假设。
首先讨论第二个假设，即避免无限多幕样本序列假设。一般的方法是”就地更新“，即在单个状态中交替进行策略的改进与评估。使用这个思路的算法称为”基于试探性出发的蒙特卡洛（蒙特卡洛ES）“
#### 同轨策略 On Policy
同轨策略使得蒙特卡洛控制不再依赖于试探性出发假设。其使用软性策略（如$\epsilon$-贪心），将贪心策略的原生概念替换到$\epsilon$-软性策略上，这样就不再需要试探性出发假设了。
#### 离轨策略 Off Policy
同轨策略本质上是并不学习策略的最优值，而是学习一个接近最优方法但仍能保持探索的策略的动作值。一个更加直接的方法是干脆使用两个策略，**目标策略**$\pi$用来学习最优解，**行动策略**$b$用来生成行动样本，这个过程被称为离轨策略学习。
覆盖假设表明，每个在$\pi$中有可能发生的动作必须在b中有可能发生。
**重要度采样**是一种在给定其他分布样本的条件下，估计某种分布期望值的通用方法。具体的，对回报值根据其轨迹在目标策略与行动策略中出现的相对概率进行加权，这个相对概率被称为**重要度采样比**。
![[attachments/Pasted image 20251112154028.png]]
此外，由于普通重要度采样的方差无限大，不易收敛，我们使用加权重要度采样算法，其方差存在上限。虽然加权重要度采样并不是无偏估计，但是最终会收敛到无偏。
由于蒙特卡洛⽅法是基于⽚段学习的，所以可以以⽚段为单位进⾏**增量式学习**，节省不必要的内存、计算资源消耗。
## 第6章 时序差分学习
### 1. 时序差分预测
对于控制问题（找到最优策略），DP，TD和蒙特卡罗方法都使用了广义策略迭代(GPI)的某个变种。这些方法的主要区别在于他们解决预测问题的不同方式。
简单来说，TD方法在状态转移到$S_{t+1}$并收到$R_{t+1}$的收益时就会做出如下更新：
$$V(S_{t})\leftarrow V(S_t) + \alpha[R_{t+1}+\gamma V(S_{t+1})-V(S_{t})]$$
时序差分学习的一个优势是它能从猜测中学习另一个猜测，即能够自举。
### 2. 时序差分控制
#### Sarsa: 同轨策略下的时序差分控制
同样的定理也适用于状态-动作二元组，即
$$Q(S_t, A_t)\leftarrow Q(S_t, A_t)+\alpha[R_{t+1}+\gamma Q(S_{t+1}, A_{t+1})-Q(S_t, A_t)]$$
如果将基于探索的$Q(S{t+1}, A_{t+1})$换成期望，就得到了**期望Sarsa**
#### Q-Learning: 离轨策略下的时序差分控制
$$Q(S_t, A_t)\leftarrow Q(S_t,A_t)+\alpha[R_{t+1}+\gamma max_aQ(S_{t+1},a)-Q(S_t,A_t)]$$
#### 最大化偏差与双学习
直接的Q-Learning存在最大化偏差的问题，因为其使用动作价值的最大值进行更新，忽视了其他值，因此存在一个显著的正偏差。可以用double learning的方法来解决。
维护两个对真是价值$q*(a)$的估计$Q_1(a)$和$Q_2(a)$，有50%的概率使用$Q_2$选择的策略更新$Q_1$，50%的概率反过来。
这个思想对所有的MDP问题都适用。
此外，还可以用批量更新的方法来缓解单次采样偏差问题。
## 第7章 n步自举
