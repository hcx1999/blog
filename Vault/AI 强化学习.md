# 强化学习
Sutton RLBook
## 第1章 导论
强化学习(Reinforcement Learning)是一种跟人类一样，通过环境的交互和反馈来进行学习。
### 1. 马尔可夫过程
![强化学习环境示意图](attachments/Pasted%20image%2020250608103854.png)

### 2. 早期历史
- 第一条主线是源于动物心理学的试错法
- 第二条主线是关注最优控制问题以及使用价值函数和动态规划的解决方案。
- 第三条主线是时序差分方法。
## 第2章 多臂老虎机
多臂老虎机是经典的强化学习模型，其特点在于无监督学习（自主学习），只依赖环境的反馈。
### 1. 问题建模
- 通过价值估计动作的选择，称为”动作-价值方法“
$$A_t \doteq argmax_a Q_t(a)$$
- 设计增量式公式以小而恒定的计算来更新平均值
$$Q_{n+1} = Q_n + \frac1n[R_n-Q_n]$$
	其中$R_n$是第n次的收益
### 2. 算法及优化
- 纯贪心、$\epsilon$-贪心
- 乐观初值
- UCB(Upper Confidence Bound)
- 梯度强盗算法
## 第3章 有限马尔可夫决策过程
相比于老虎机只需要关注即时收益，大多数问题更需要关心延迟收益。对于这些问题，我们可以使用MDP进行建模。
### 1. 有限MDP(Markov Decision Procedure)
![[attachments/Pasted image 20251006142310.png]]
- 目标被形式化表征为一种特殊信号，称为收益。
在这个模型下，收益可以被表示为：
$$p(s', r|s, a)\doteq Pr\{S_t=s', R_t=r|S_{t-1}=s, A_{t-1}=a\}$$
其中Pr表示Probability。
- 我们希望最大化回报，而回报与未来的收益序列有关。
一般将回报定义为
$$G_t \doteq R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\dots=\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}$$
$$G_t\doteq R_{t+1}+\gamma G_{t+1}$$
其中$\gamma$称为折扣率
- 智能体和环境的交互具有马尔可夫性，有限MDP中下一个状态仅由有限个状态决定。我们可以用状态转移概率表示。
$$p(s'|s, a)\doteq \mathbb{E}[R_t|S_{t-1}=s, A_{t-1}=a]=\sum_{r\in \mathcal{R}}p(s', r|s, a)$$
- 我们还可以定义“状态-动作”二元组的期望收益，并将其表示为一个双参数函数：
$$r(s, a)\doteq E[R_t|S_{t-1}=s, A_{t-1}=a]$$
和“状态-动作-后继状态”三元组的期望收益，并将其表示为一个三参数函数：
$$r(s, a, s')\doteq \mathbb{E}[R_t|S_{t-1}=s, A_{t-1}=a, S_t=s']=\sum_{r\in \mathcal{R}}r\frac{p(s', r|s, a)}{p(s'|s, a)}$$
- MDP框架将智能体与环境之间传递的所有信息看作3个信号：行动、状态、收益。
任务分为分幕式任务和持续性任务。我们定义“吸收态”来统一这两种类型的任务。吸收态只能转移到自己且收益为0。这样就只需要研究分幕式任务。而在分幕式任务中，不必区分不同的幕，$S_t$表示的是每一幕中t时刻的状态。
### 2. 策略与价值
- 策略：状态到可行的动作的一个分布函数，用$\pi(a|s)$表示$S_t = s$时$A_t = a$的概率。而强化学习就是通过策略来修改$\pi$。此外，我们用$v_\pi(s)$来表示策略$\pi$的状态价值，用$q_\pi(s)$来表示策略$\pi$的动作价值。
![[attachments/Pasted image 20251007131155.png]]
可以通过蒙特卡洛方法计算这两个值。（算出来、存起来）
#### 贝尔曼方程
原理：当智能体采取策略$\pi$，并对于每个状态都记录实际回报的平均值，那么当状态出现的次数趋于无穷时，这个平均值就会收敛到$v_{\pi}(s)$，同理动作的回报的平均值收敛到$q_{\pi}(s, a)$。
于是我们可以用蒙特卡洛方法来求出这两者的值。

具体的，贝尔曼期望方程描述了任何策略$\pi$和任何状态s，s的价值与其后继状态价值的关系。
另外，现在我们研究的是一个既定策略的价值，但我们期望得到的是一个最优策略。**贝尔曼最优方程**阐述了一个事实：最优状态下各个状态的价值一定等于这个状态下最优动作的期望价值。
![[attachments/Pasted image 20251007133840.png]]
解释：**贝尔曼期望方程**来源于$v_{\pi}(x)$的定义，第一个Sigma是对当前状态每种动作的求和，第二个Sigma是对给定动作可能结果的求和，求和的内容是回报的加权平均。
**贝尔曼最优方程**将期望方程中的第一个Sigma换成了max，这意味着v不再收敛到给定策略的价值，而是收敛到整个问题的最优策略的价值。

贝尔曼方程的意义在于，我们可以将最优的长期（全局）回报期望值转化为每个状态对应的一个当前局部量的计算。动作价值函数$q*(s, a)$包含着所有单步搜索的结果，智能体只需要找到q* ，最优动作的选取就不再需要知道后继状态及其对应的价值了，也就是说不需要再与动态环境进行交互了。

理论上，贝尔曼方程组有n个方程和n个未知数，可以直接解出最优策略。但现实问题往往无法求出闭式解，我们更关注问题的近似解。

## 第4章 动态规划
> In fact, all of these methods can be viewed as attempts to achieve much the same effect as DP, only with less computation and without assuming a perfect model of the environment.
 ——RLBook
 
 在强化学习中，DP的核心思想是使用价值函数来结构化的组织对最优策略的探索。
### 1. 通过策略改进得到最优策略
1. 策略评估 Policy Evaluation
	我们希望对于策略$\pi$计算$v_{\pi}$:
	- 迭代策略评估：通过迭代近似的价值函数序列得到贝尔曼方程的近似解。在保证$v_\pi$存在的条件下，序列$v_k$将会在$k\rightarrow\infty$时收敛到$v_\pi$。
	- 期望更新：通过单步转移后的即时收益和$s$的每个状态（或状态-动作二元组）的旧的价值函数二者的期望值来更新$s$的新的价值函数。
2. 策略改进 Policy Improvement
	我们可以通过策略评估进行策略改进，从而确定最优策略。
3. 策略迭代 Policy Iteration
	- 策略迭代：交替进⾏迭代策略估值和策略提升，在有限步之后找到最优策略与最优值函数。
### 2. 通过最最优状态价值得到最优策略
1. 值迭代
	