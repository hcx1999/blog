# 强化学习
Sutton RLBook + Tencent AIArena
## 第1章 导论
强化学习(Reinforcement Learning)是一种跟人类一样，通过环境的交互和反馈来进行学习。
### 1. 马尔可夫过程
![强化学习环境示意图](attachments/Pasted%20image%2020250608103854.png)

### 2. 早期历史
- 第一条主线是源于动物心理学的试错法
- 第二条主线是关注最优控制问题以及使用价值函数和动态规划的解决方案。
- 第三条主线是时序差分方法。
## 第2章 多臂老虎机
多臂老虎机是经典的强化学习模型，其特点在于无监督学习（自主学习），只依赖环境的反馈。
### 1. 问题建模
- 通过价值估计动作的选择，称为”动作-价值方法“
$$A_t \doteq argmax_a Q_t(a)$$
- 设计增量式公式以小而恒定的计算来更新平均值
$$Q_{n+1} = Q_n + \frac1n[R_n-Q_n]$$
	其中$R_n$是第n次的收益
### 2. 算法及优化
- 纯贪心、$\epsilon$-贪心
- 乐观初值
- UCB(Upper Confidence Bound)
- 梯度强盗算法
## 第3章 有限马尔可夫决策过程
相比于老虎机只需要关注即时收益，大多数问题更需要关心延迟收益。对于这些问题，我们可以使用MDP进行建模。
### 1. 有限MDP(Markov Decision Procedure)
![[attachments/Pasted image 20251006142310.png]]
- 目标被形式化表征为一种特殊信号，称为收益。
在这个模型下，收益可以被表示为：
$$p(s', r|s, a)\doteq Pr\{S_t=s', R_t=r|S_{t-1}=s, A_{t-1}=a\}$$
其中Pr表示Probability。
- 我们希望最大化回报，而回报与未来的收益序列有关。
一般将回报定义为
$$G_t \doteq R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\dots=\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}$$
$$G_t\doteq R_{t+1}+\gamma G_{t+1}$$
其中$\gamma$称为折扣率
- 智能体和环境的交互具有马尔可夫性，有限MDP中下一个状态仅由有限个状态决定。我们可以用状态转移概率表示。
$$p(s'|s, a)\doteq \mathbb{E}[R_t|S_{t-1}=s, A_{t-1}=a]=\sum_{r\in \mathcal{R}}p(s', r|s, a)$$
- 我们还可以定义“状态-动作”二元组的期望收益，并将其表示为一个双参数函数：
$$r(s, a)\doteq E[R_t|S_{t-1}=s, A_{t-1}=a]$$
和“状态-动作-后继状态”三元组的期望收益，并将其表示为一个三参数函数：
$$r(s, a, s')\doteq \mathbb{E}[R_t|S_{t-1}=s, A_{t-1}=a, S_t=s']=\sum_{r\in \mathcal{R}}r\frac{p(s', r|s, a)}{p(s'|s, a)}$$
- MDP框架将智能体与环境之间传递的所有信息看作3个信号：行动、状态、收益。
任务分为分幕式任务和持续性任务。我们定义“吸收态”来统一这两种类型的任务。吸收态只能转移到自己且收益为0。这样就只需要研究分幕式任务。而在分幕式任务中，不必区分不同的幕，$S_t$表示的是每一幕中t时刻的状态。
### 2. 策略与价值
- 策略：状态到可行的动作的一个分布函数，用$\pi(a|s)$表示$S_t = s$时$A_t = a$的概率。而强化学习就是通过策略来修改$\pi$。此外，我们用$v_\pi(s)$来表示策略$\pi$的状态价值，用$q_\pi(s)$来表示策略$\pi$的动作价值。
![[attachments/Pasted image 20251007131155.png]]
可以通过蒙特卡洛方法计算这两个值。（算出来、存起来）
#### 贝尔曼方程
原理：当智能体采取策略$\pi$，并对于每个状态都记录实际回报的平均值，那么当状态出现的次数趋于无穷时，这个平均值就会收敛到$v_{\pi}(s)$，同理动作的回报的平均值收敛到$q_{\pi}(s, a)$。
于是我们可以用蒙特卡洛方法来求出这两者的值。

具体的，贝尔曼期望方程描述了任何策略$\pi$和任何状态s，s的价值与其后继状态价值的关系。
另外，现在我们研究的是一个既定策略的价值，但我们期望得到的是一个最优策略。**贝尔曼最优方程**阐述了一个事实：最优状态下各个状态的价值一定等于这个状态下最优动作的期望价值。
![[attachments/Pasted image 20251007133840.png]]
解释：**贝尔曼期望方程**来源于$v_{\pi}(x)$的定义，第一个Sigma是对当前状态每种动作的求和，第二个Sigma是对给定动作可能结果的求和，求和的内容是回报的加权平均。
**贝尔曼最优方程**将期望方程中的第一个Sigma换成了max，这意味着v不再收敛到给定策略的价值，而是收敛到整个问题的最优策略的价值。

贝尔曼方程的意义在于，我们可以将最优的长期（全局）回报期望值转化为每个状态对应的一个当前局部量的计算。动作价值函数$q*(s, a)$包含着所有单步搜索的结果，智能体只需要找到q* ，最优动作的选取就不再需要知道后继状态及其对应的价值了，也就是说不需要再与动态环境进行交互了。

理论上，贝尔曼方程组有n个方程和n个未知数，可以直接解出最优策略。但现实问题往往无法求出闭式解，我们更关注问题的近似解。

## 第4章 动态规划
> In fact, all of these methods can be viewed as attempts to achieve much the same effect as DP, only with less computation and without assuming a perfect model of the environment.
 ——RLBook
 
 在强化学习中，DP的核心思想是使用价值函数来结构化的组织对最优策略的探索。
### 1. 通过策略改进得到最优策略
1. 策略评估 Policy Evaluation
	我们希望对于策略$\pi$计算$v_{\pi}$:
	- 迭代策略评估：通过迭代近似的价值函数序列得到贝尔曼方程的近似解。在保证$v_\pi$存在的条件下，序列$v_k$将会在$k\rightarrow\infty$时收敛到$v_\pi$。
	- 期望更新：通过单步转移后的即时收益和$s$的每个状态（或状态-动作二元组）的旧的价值函数二者的期望值来更新$s$的新的价值函数。
2. 策略改进 Policy Improvement
	我们可以通过策略评估进行策略改进，策略改进定理表示每一次改进都不会比先前更差，由状态的有限性可以得到策略改进算法一定可以在有限步内收敛。
3. 策略迭代 Policy Iteration
	- 策略迭代：交替进⾏迭代策略估值和策略提升，在有限步之后找到最优策略与最优值函数。
### 2. 通过最最优状态价值得到最优策略
1. 值迭代
	- 是一种提升策略迭代效率的方法。策略迭代在策略评估上浪费了大量时间，我们想到可以提前结束估值。
	- 值迭代是一种极端的矫正方式，只做一次估值就开始策略提升。
	- 实际问题中可以将两者结合，每值迭代若干轮做一次策略提升。
### 3. 提升求解效率
1. 异步动态规划
	![[attachments/Pasted image 20251017103144.png]]
2. 广义策略迭代 Generalized Policy Iteration(GPI)
	- 几乎所有的强化学习方法都可以用GPI描述。
	- 与过程的粒度或其他细节无关。
	![[attachments/Pasted image 20251017103705.png]]
		3. 动态规划效率
		- 对初始值依赖性比较大
		- 假设环境已知(P, R已知)
		- 使用bootstrap（状态的估值是根据其后继状态估值的）
## 第5章 蒙特卡洛方法
蒙特卡洛方法不假设有完备的环境知识，而是只需要经验。
### 1. 蒙特卡洛预测和价值评估
- 蒙特卡洛预测有一个重要前提：对每个状态的估计是独立的。
- 如果无法得到环境模型，那么计算动作价值就比计算状态价值更加有用一些。我们可以用状态-动作二元组的价值$q_\pi(s, a)$来表示动作价值。
- 直接蒙特卡洛会遇到问题，一部分状态可能永远没有被访问到。
	1. 引入“探索性起步”，指定开始采样的状态，这样可以保证每个状态都收敛。
	2. 保证每个状态下所有动作都有非零概率被选中，也就是$\epsilon$-soft策略。
### 2. 蒙特卡洛控制
#### 蒙特卡洛ES
”试探性出发“假设是指在采样次数趋向于无穷时，每个状态-动作二元组都会被访问无数次。
另外一个假设是在进行策略评估时有无限多幕的样本序列进行试探。
满足这两个假设就可以直接使用蒙特卡洛算法，但大多数情况下不满足，因此我们需要用某些方法来去除这两个假设。
首先讨论第二个假设，即避免无限多幕样本序列假设。一般的方法是”就地更新“，即在单个状态中交替进行策略的改进与评估。使用这个思路的算法称为”基于试探性出发的蒙特卡洛（蒙特卡洛ES）“
#### 同轨策略 On Policy
同轨策略使得蒙特卡洛控制不再依赖于试探性出发假设。其使用软性策略（如$\epsilon$-贪心），将贪心策略的原生概念替换到$\epsilon$-软性策略上，这样就不再需要试探性出发假设了。
#### 离轨策略 Off Policy
同轨策略本质上是并不学习策略的最优值，而是学习一个接近最优方法但仍能保持探索的策略的动作值。一个更加直接的方法是干脆使用两个策略，**目标策略**$\pi$用来学习最优解，**行动策略**$b$用来生成行动样本，这个过程被称为离轨策略学习。
覆盖假设表明，每个在$\pi$中有可能发生的动作必须在b中有可能发生。
**重要度采样**是一种在给定其他分布样本的条件下，估计某种分布期望值的通用方法。具体的，对回报值根据其轨迹在目标策略与行动策略中出现的相对概率进行加权，这个相对概率被称为**重要度采样比**。
![[attachments/Pasted image 20251112154028.png]]
此外，由于普通重要度采样的方差无限大，不易收敛，我们使用加权重要度采样算法，其方差存在上限。虽然加权重要度采样并不是无偏估计，但是最终会收敛到无偏。
由于蒙特卡洛⽅法是基于⽚段学习的，所以可以以⽚段为单位进⾏**增量式学习**，节省不必要的内存、计算资源消耗。
## 第6章 时序差分学习
### 1. 时序差分预测
对于控制问题（找到最优策略），DP，TD和蒙特卡罗方法都使用了广义策略迭代(GPI)的某个变种。这些方法的主要区别在于他们解决预测问题的不同方式。
简单来说，TD方法在状态转移到$S_{t+1}$并收到$R_{t+1}$的收益时就会做出如下更新：
$$V(S_{t})\leftarrow V(S_t) + \alpha[R_{t+1}+\gamma V(S_{t+1})-V(S_{t})]$$
时序差分学习的一个优势是它能从猜测中学习另一个猜测，即能够自举。
### 2. 时序差分控制
#### Sarsa: 同轨策略下的时序差分控制
同样的定理也适用于状态-动作二元组，即
$$Q(S_t, A_t)\leftarrow Q(S_t, A_t)+\alpha[R_{t+1}+\gamma Q(S_{t+1}, A_{t+1})-Q(S_t, A_t)]$$
如果将基于探索的$Q(S{t+1}, A_{t+1})$换成期望，就得到了**期望Sarsa**
#### Q-Learning: 离轨策略下的时序差分控制
$$Q(S_t, A_t)\leftarrow Q(S_t,A_t)+\alpha[R_{t+1}+\gamma max_aQ(S_{t+1},a)-Q(S_t,A_t)]$$
#### 最大化偏差与双学习
直接的Q-Learning存在最大化偏差的问题，因为其使用动作价值的最大值进行更新，忽视了其他值，因此存在一个显著的正偏差。可以用double learning的方法来解决。
维护两个对真是价值$q*(a)$的估计$Q_1(a)$和$Q_2(a)$，有50%的概率使用$Q_2$选择的策略更新$Q_1$，50%的概率反过来。
这个思想对所有的MDP问题都适用。
此外，还可以用批量更新的方法来缓解单次采样偏差问题。
## 第七、八讲 基于价值的近似函数方法
之前的学习方法数学上看起来很美妙，但是在现实中存在一个致命问题，就是现实的状态往往多到无法穷举。当状态过多或状态转移过多时，我们希望得到一个近似的函数来估计状态价值。这里我们采用最好用的一种——神经网络。
采取神经网络进行训练的强化学习方法被称为深度强化学习。他与传统深度学习即监督学习的核心区别是使用不断累积的采样数据而不是给定的训练数据，同时采样的策略和目标函数也一直在变化。我们使用价值的**均方误差损失函数**作为深度学习训练的损失函数。
### 随机梯度下降预测
由于我们的采样是随机的，所以损失函数的梯度也是随机的。这种梯度下降的方法称为随机梯度下降法。采样的重要度可以用蒙特卡洛估计，进一步优化可以使用n-step TD或TD(λ)进行估计。
### 轨迹式半梯度控制
![[attachments/Pasted image 20251225043228.png]]

半梯度随机下降方法适用于无限采样的深度强化学习，更容易收敛。
现在我们根据半梯度预测来对策略进行控制。首先可以想到的是朴素的**n步轨迹式半梯度SARSA**，这种方法顾名思义使用半梯度更新近似价值函数，然后根据价值函数进行n-step SARSA。
进一步的，对于没有终止状态的持续问题，我们引入**平均回报**来解决。计算平均回报的方法称为**差分回报**，平均回报的特性称为**持续性**。得到的最终算法称为**n步差分半梯度SARSA**。
### DQN
DQN即Deep Q-learning Network，是一种基于价值函数近似的在线Q-learning算法。他采取两种关键技术：**经验回放**、**目标网络**。
- 经验回放：小批量随机回放之前一段时间的样本进行重新训练。打破了数据间的时序相关性，提高了数据利用率。
- 目标网络：固定了TD的目标值。使得训练能够收敛。
DQN可以进行改进：
- Double DQN：解决DQN因高估传播的问题。使用两个价值函数（即两个网络）评估后续价值，可以环节DQN因自己当前的错误估计对后续状态的估计错上加错的缺点。
- Dueling DQN：将Q值分解为状态价值$V(s)$和动作优势$A(s,a)$，环境了学习效率与泛化能力不足的问题。
- 优先回放经验池：经验回放打破数据相关性，而优先经验回放找回一部分数据相关性，给回放样本赋予优先级来提高效率。
- Rainbow消融实验：多个提升方法逐个消融，对比试验，看看哪个提升方法的提升最大（缺了哪个的影响最大qwq）
## 第九、十讲：基于策略的学习
与基于价值的方法不同，基于策略的方法采取采样价值而非对价值取argmax的方法来确定策略，可以处理动作空间连续等现实情况。
### 蒙特卡洛策略梯度算法 REINFORCE
首先想到朴素的蒙特卡洛策略采样。方差大收敛慢。然后对其进行改进：
- **REINFORCE with Baseline**：加入基线值b(s)，是s的动作价值的均值。这样可以将采样均值变为0，减小方差，属于一种正则化方法。
### Actor-Critic方法
将REINFORCE的蒙特卡洛采样替换为TD，即可得到朴素Actor-Critic。
- **A2C**：AC with baseline
- **A3C**：多个worker采样，进行异步更新。是一项工程优化。
- **SAC**：普通SAC寻找一个策略最大化累计期望值，未指定策略固定or随机。而SAC基于最大熵强化学习框架，旨在最大化策略熵，这意味着SAC在追求高回报的基础上鼓励探索。此外，SAC使用**重参数化技巧**，将动作分布分解成**固定的与参数无关的随机源+参数决定的确定性变换**。这将原来不可导的、不确定的策略分布直接转化成了可导的、确定的。确定的策略提升了稳定性，最大熵RL保证了探索性，对分布求梯度而非直接采样减小了方差。
- **有限差分方法**：**有限差分方法** 在强化学习中特指一种 **“零阶优化”** 或 **“黑箱优化”** 方法，用于估计价值函数的梯度或直接优化策略参数（代替用策略梯度优化策略函数）。他通过直接扰动参数并观察输出变化来估计梯度或寻找最优参数。这种方法在今天的RL中几乎没有使用，而是使用他的一种近亲：**进化算法**。
### TRPO和PPO
**重要性采样**是一种**用已知分布的样本来估计另一个分布期望值**的技术。我们想估计函数 $f(x)$ 在分布 $p(x)$ 下的期望：$𝔼_{x∼p}[f(x)]$，但我们只有从分布 $q(x)$ 中采样的样本 $x_i ∼ q$。通过重要性采样重写：$𝔼_{x∼p}[f(x)] = 𝔼_{x∼q}[ (p(x)/q(x)) * f(x) ] = 𝔼_{x∼q}[ ρ(x) * f(x) ]$，其中 $ρ(x) = p(x)/q(x)$ 称为**重要性权重**。**重要性采样的本质**是校正分布差异，让旧数据能用于评估新策略。
#### TRPO
**TRPO(Trust Region Policy Optimization，信任域策略优化)** 是指在更新策略时添加一个阈值，避免新旧策略差异过大导致一次更新就瘫痪。我们用**重要性采样**确定新策略的分布，然后用**KL散度**衡量新旧策略之间的差异。
**替代目标函数**是TRPO和PPO的核心思想。因为直接使用**策略的期望累计回报**无法求梯度来确定新策略，故使用重要性采样的数据给出一个替代的函数。注意这个替代函数代替的是目标函数也就是策略的期望**累计回报**，而不是状态价值函数、动作价值函数或优势函数。
TRPO的缺点是信任域约束的具体实现方法需要复杂二阶优化，如自然梯度或共轭梯度法。计算成本高，工程上不友好。
#### PPO
**PPO(Proximal Policy Optimization，近端策略优化)** 放弃了使用严格的信任域对KL散度进行限制。而是替换成了别的机制。
**PPO-penalty**将KL散度的硬约束变成了软约束。在loss上加入惩罚项。但还是要算KL散度。
**PPO-clip**直接抛弃了KL散度，反正使用替代目标函数，只要策略变化不大就行。PPO-clip直接使用重要性采样比控制step步长，对于步长超过阈值$\epsilon$的代理目标函数直接截断，这种简单粗暴的方法称为**截断机制(PPO-clip)**，得到的新代理目标函数称为**裁剪代理目标**。
PPO-clip避免了重要性权重$\rho$过大导致梯度爆炸或消失的问题。同时可以使用Adam等一阶优化器进行优化，计算更简单，工程友好。缺点是没有TRPO具有的**策略性能单调改进的理论保证**，即新策略的期望回报一定不小于旧策略。
**GAE(Generalized Advantage Estimation，广义优势估计)** 是用于估计策略学习中优势函数`A(s,a)`的方法。具体使用的就是普通的**TD(λ)**。值得一提的是，朴素的advantage估计是基于TD Error的，这种方法虽然方差较低，但是引入了偏差。GAE通过调节λ实现了方差和偏差的trade-off。
#### DPPO
**分布式PPO**：在PPO的基础上采用多worker并行的收集轨迹并计算梯度，然后chief将所有的梯度计算平均，并返回给worker更新其策略。
#### DDPG
![[attachments/Pasted image 20251225132236.png]]
