# AI基础03-神经网络
## 1. 单个神经元
输入层(input layer)、输出层(output layer)、权重(weight)、偏置(bias)
其中权重是矩阵，输入、输出、偏置都是向量。
偏置(bias)在分类(classification)问题中起到让决策边界(decision boundary)可以不经过原点的作用。
## 2. 激活函数
作用：在模型中增加非线性的属性。
#### Sigmoid函数
- 表达式：$\sigma(z) = \frac{1}{1+e^{-z}}$
- 映射关系：$(-\infty, +\infty)\rightarrow(0, 1)$输出值在 0 到 1 之间，用以表示概率
#### Tanh函数
- 公式： $f ( x ) = \frac { e ^ { x } - e ^ { - x } } { e ^ { x } + e ^ { - x } } = \frac { 2 } { 1 + e ^ { - 2 x } }$ 
- 映射关系： $( - \infty , + \infty ) \rightarrow ( - 1 , 1 ) ,$ 输出值在-1到1之间，常用于回归任务
#### ReLU函数
- 公式 $f ( x ) = \max ( 0 , x )$ 
- 映射关系： $( - \infty , + \infty ) \rightarrow ( 0 , + \infty ) ,$ 最常用的分类函数，可以用于特征提取、**简化网络优化**（缓解梯度消失问题、偏导数好计算) 
#### Leaky ReLU函数
公式： $f ( x ) = \max ( \alpha x , x )$ 
映射关系： $( - \infty , + \infty ) \rightarrow ( - \infty , + \infty ) ,$ 解决R e L U 函数中负数部分输出为0的问题，其中的 $\alpha$ 是一个小的常数，如0.01，在ParametricReLU 中，这个 $\alpha$ 是一个可学习的参数。
#### Softmax函数
公式： $f \left( x _ { i } \right) = \frac { e ^ { x _ { i } } } { \sum _ { j = 1 } ^ { n } e ^ { x _ { j } } }$ 
映射关系： $( - \infty , + \infty ) \rightarrow ( 0 , 1 ) ,$ 输出值在0到1之间，用以表示概率，使得所有激活值之和为1.
## 3. 多层感知器
解决XOR异或分类问题，可以通过添加一个隐藏层来解决。
一般使用上标表示层序号，用下标表示该层的神经元序号。
> 多层感知器可以进一步抽象成 Encoder-Decoder 模型，其中 Encoder 用于提取特征，Decoder 用于还原数据。
## 4. 损失函数
#### 逻辑回归（分类问题）：交叉熵损失函数（Cross-Entropy Loss）
- 交叉熵非负
- 交叉熵=真实熵+KL散度
- 交叉熵是不对称的
#### 线性回归（回归问题）：均方误差损失函数（Mean Squared Error）
MSE使用L2范数进行正则化。
## 5. 优化
默认使用**梯度下降(Gradient Descent)**->需要求出梯度$\frac{\partial \mathcal L}{\partial \theta}$->使用误差反向传播(Error Back-Propagation)计算所有参数的梯度。
#### 误差反向传播
误差反向传播是用来计算神经网络中所有参数的梯度$\frac{\partial \mathcal L}{\partial \theta}$的方法，计算梯度时，引入误差函数对输出值$z$的偏导$\delta = \frac{\partial\mathcal L}{\partial z}$作为中间结果，并用这个中间结果计算每一层的梯度。
具体咋求的我也没听懂，反正可以用$\delta$来求梯度。可见pptLecture5P58.
#### 梯度消失和梯度爆炸
**梯度消失(Gradient Vanish)** 问题是指反向传播的过程中中间结果$\delta$变得很小导致靠近输入层的参数无法更新的问题。解决方法一般是用ReLU(LeakyReLU, ELU等)函数代替Sigmoid函数。
#### 随机梯度下降
**随机梯度下降(Stochastic Gradient Descent)** 每次用所有的训练样本来计算误差$\mathcal{L}$会很慢，所以每次更新$\mathcal{L}$时从中随机选取一批(batch)数据来计算误差，这批数据被称为mini-batch。
具体的，我们随机选取一批数据计算均值，再以此均值为随机梯度下降的方向。这样做有利于充分利用硬件资源，减少单样本采样导致的抖动。
缺点：容易陷入局部最优解。
#### 学习率调度
自适应学习率，有Adagrad、RMSprop、Adam等，都是SGD的变种。
#### 超参数选择(Hyper-Parameter Selection)
一般来说，可以把训练集数据分为训练数据和验证数据(Validation Data)。
对于数据稀少的情况，可以使用K折交叉验证(K-Fold Cross Validation)，是吧整个数据集平均分成K份，每次取一个做测试集其余做训练集。
## 6. 正则化
广义的正则化泛指一切可以减少过拟合(overfitting)的方法。
#### 数据增强
水平对称翻转(horizental flipping)、旋转(rotating)、平移(shifting)、缩放(zooming)等
#### 提前停止
当网络开始过拟合时提前停止训练
#### 权重衰减
其思想是在损失函数中添加损失项来限制权重的大小，从而减小模型复杂度
$\mathcal{L}_{total} = \mathcal{L} + \lambda||W||$
权重衰减只用于权重weight，不适用于偏置bias
#### $\mathcal{L}1$和$\mathcal{L}2$正则化
作为权重衰减的惩罚项，$\mathcal L 1$稀疏，$\mathcal L 2$平滑。
#### Dropout
随机丢弃一部分神经元（对隐藏输出置0）
注意模型测试/正常使用时就不要再Dropout了。